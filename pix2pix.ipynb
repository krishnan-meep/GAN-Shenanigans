{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pix2pix\n",
    "\n",
    "UNet-Generator with Resblocks in the middle, Basic Discriminators but the PatchGAN option is available, Least Squares loss + L1 reconstruction loss\n",
    "Conditioning in the discriminator is done by concatenating the images from domain x and y together and feeding it to the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from models.cycgan import UNet_Generator, Basic_Discriminator\n",
    "from utils import load_data\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coloured images (Y)....\")\n",
    "y_train = load_data(path=\"/content/drive/My Drive/Datasets/Fruits\", image_size = (128,128), block_size = 1500)\n",
    "print(\"BW images (X)....\")\n",
    "x_train = load_data(path=\"/content/drive/My Drive/Datasets/Fruits\", image_size = (128,128), block_size = 1500, \n",
    "                    as_grayscale = True)\n",
    "\n",
    "y_train = (y_train/255)*2 - 1\n",
    "x_train = (x_train/255)*2 - 1\n",
    "\n",
    "print(x_train.max(), x_train.min(), y_train.max(), y_train.min())\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_x = torch.utils.data.DataLoader(x_train, batch_size=64, num_workers=2, shuffle = True)\n",
    "train_loader_y = torch.utils.data.DataLoader(y_train, batch_size=64, num_workers=2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = iter(train_loader_x)\n",
    "data = next(data_loader)\n",
    "\n",
    "print(\"x_data\")\n",
    "print(data.shape)\n",
    "print(data.max(), data.min())\n",
    "img = np.transpose(data[0], (1, 2, 0))\n",
    "plt.imshow((img.reshape(128, 128)+1)/2, cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "data_loader = iter(train_loader_y)\n",
    "data = next(data_loader)\n",
    "\n",
    "print(\"y_data\")\n",
    "print(data.shape)\n",
    "print(data.max(), data.min())\n",
    "img = np.transpose(data[0], (1, 2, 0))\n",
    "plt.imshow((img+1)/2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G = UNet_Generator(in_channels = 1).to(device)\n",
    "D = Basic_Discriminator(in_channels = 4).to(device)\n",
    "\n",
    "#Orthogonal initialization is king\n",
    "for m in G.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):\n",
    "        nn.init.orthogonal_(m.weight)\n",
    "\n",
    "for m in D.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):\n",
    "        nn.init.orthogonal_(m.weight)\n",
    " \n",
    "#Optimizers\n",
    "optimizerD = optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "_lambda = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (data_x, data_y) in enumerate(zip(train_loader_x, train_loader_y)):\n",
    "\n",
    "        #Dealing with the discriminators################################\n",
    "        D.zero_grad()\n",
    "\n",
    "        real_images_x = data_x.to(device)\n",
    "        real_images_y = data_y.to(device)\n",
    "        \n",
    "        b_size = real_images_x.size(0)\n",
    "        #Concatenate image x as a condition image y, conditional discriminator yes?\n",
    "        input_images = torch.cat([real_images_x, real_images_y], dim = 1)\n",
    "  \n",
    "        output = D(input_images).view(-1)\n",
    "        errD_real = torch.mean((output - 1)**2)\n",
    "\n",
    "        #concatenate x with the fake images and then feed it to the discriminator\n",
    "        fake_images_y = G(real_images_x)\n",
    "        input_images = torch.cat([real_images_x, fake_images_y], dim = 1)\n",
    "        \n",
    "        output = D(input_images.detach()).view(-1)\n",
    "        errD_fake = torch.mean((output)**2)\n",
    "\n",
    "        errD = errD_fake + errD_real\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        #Dealing with the generators###################################\n",
    "        G.zero_grad()\n",
    "        \n",
    "        output = D(input_images).view(-1)\n",
    "        \n",
    "        errG_adv = torch.mean((output - 1)**2) \n",
    "        errG_cyc = torch.mean(torch.abs(fake_images_y - real_images_y))\n",
    "        errG_cyc *= _lambda\n",
    "        \n",
    "        errG = errG_adv + errG_cyc\n",
    "        errG.backward()\n",
    "\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"Epoch %i Step %i --> Disc_Loss : %f   Gen_Loss : %f\" % (epoch, i, errD, errG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = np.random.choice(len(x_train), size = 10)\n",
    "data_x = x_train[batch_idx]\n",
    "\n",
    "print(\"Actual images\")\n",
    "\n",
    "f, a = plt.subplots(1, 10, figsize=(20, 20))\n",
    "for i in range(10):\n",
    "  img = data_x[i]\n",
    "  img = np.transpose(img, (1, 2, 0))\n",
    "  img = (img.reshape(128,128)+1)/2\n",
    "  a[i].imshow(img, cmap = \"gray\")\n",
    "  a[i].axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "  real_images_x = torch.Tensor(data_x).to(device)\n",
    "\n",
    "  fake_images_y = G(real_images_x)\n",
    "\n",
    "print(\"Translated images\")\n",
    "\n",
    "f, a = plt.subplots(1, 10, figsize=(30, 30))\n",
    "for i in range(10):\n",
    "  img = fake_images_y[i].cpu()\n",
    "  img = np.transpose(img, (1, 2, 0))\n",
    "  img = (img+1)/2\n",
    "  a[i].imshow(img)\n",
    "  a[i].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
